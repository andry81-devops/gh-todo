* Implement push-pull retry loop to make a git repo push from different
  workflow pipelines (otherwise a git push fails and a workflow contained it
  does fail too until the next run).

  for:

    /gh-action--*
    /gh-workflow

* Implement a github repo statistic push from different repos into a single git
  repo to avoid a github account repositories page overrun by the automated
  user commits (page updates each time a commit has made).

  for:

    /github-accum-stats
    /gh-action--*
    /gh-workflow

* Implement a repository release downloads accumulator.

    for:

      /github-accum-stats
      /gh-action--accum-repo-release-downloads
      /gh-workflow

  Count all downloads in the `Github dl|all` badge on the github repo main page
  including deleted.

* Implement a repository subscribers accumulator.

    for:

      /github-accum-stats
      /gh-action--accum-repo-subscribers
      /gh-workflow

* Implement a repository stargazers accumulator.

    for:

      /github-accum-stats
      /gh-action--accum-repo-stargazers
      /gh-workflow

* Implement an account followers accumulator.

    for:

      /github-accum-stats
      /gh-action--accum-account-followers
      /gh-workflow

* Implement a new tag watcher GitHub action experimental script to watch an
  arbitrary git repository for a tag creation by a name pattern and does
  reflect it as a release tag creation in another git repository to be able to
  notify the owner about each tag creation.
  Can be used for repositories where the owner does not make release but
  instead does make tags.

* Implement a repository events accumulator.

    for:

      /github-accum-stats
      /gh-action--accum-repo-events
      /gh-workflow

* Implement per date commit message to indicate particular statistic for a date
  in a file or directory. For example, the `by_year` directory does contain
  files in format `YYYY-MM-DD.json`. Each that file can be committed separately
  with different commit message to indicate different accumulation statistic
  segmented by date.

    for:

      /gh-workflow

* Implement a repository commits accumulator.

    for:

      /github-accum-stats
      /gh-action--accum-repo-commits
      /gh-workflow

* Implement github links patch by pattern before the
  `$GH_WORKFLOW_ROOT/bash/github/flush-print-annotations.sh`
  script if a commit at the end is executed and succeeded to replace
  all generic links like
  `https://github.com/{{OWNER}}/{{REPO}}/tree/master/...` by
  `https://github.com/{{OWNER}}/{{REPO}}/tree/{{COMMIT_HASH}}/...`.

  It will point the commit from the workflow log directly instead imprecisely
  by a date (except the changelog file which is part of a commit and so still
  has the link to the commit by a date).

* Implement a repository links accumulator and checker.
  Schedule iteration over repos and files, extract web links by patterns,
  accumulate repo/file/link paths, check links and accumulate response.

    for:

      /accum-url-links
      /check-url-links
      /gh-action--accum-url-links
      /gh-action--check-url-links
      /gh-workflow

* Implement a repository cache with bootstrapper executables periodically
  taken out by the `gh-action--accum-content` action script.

    for:

      andry81-devops/bootstrappers
      andry81-cache/bootstrappers--gh-content-cache
      /gh-workflow

    bootstrapper executables to take from:

      * https://learn.microsoft.com/en-us/previous-versions/visualstudio/visual-studio-2017/install/create-an-offline-installation-of-visual-studio?view=vs-2017
        https://aka.ms/vs/15/release/vs_professional.exe
        https://aka.ms/vs/15/release/vs_enterprise.exe
        https://aka.ms/vs/15/release/vs_buildtools.exe

      * https://learn.microsoft.com/en-us/visualstudio/install/create-an-offline-installation-of-visual-studio?view=vs-2019
        https://aka.ms/vs/16/release/vs_professional.exe
        https://aka.ms/vs/16/release/vs_enterprise.exe
        https://aka.ms/vs/16/release/vs_buildtools.exe

      * https://learn.microsoft.com/en-us/visualstudio/install/create-an-offline-installation-of-visual-studio?view=vs-2022
        https://aka.ms/vs/17/release/vs_community.exe
        https://aka.ms/vs/17/release/vs_professional.exe
        https://aka.ms/vs/17/release/vs_enterprise.exe
        https://aka.ms/vs/17/release/vs_buildtools.exe

      * https://www.cygwin.com/
        https://www.cygwin.com/setup-x86_64.exe

* Implement first commit or initial files content template in the
  `gh-action--accum-gh-stats` action script to automatically insert,
  for example, basic `README.md` file into a statistic output repository.

    for:

      /github-accum-stats
      /gh-action--accum-gh-stats
      /gh-workflow

    example of a `README.md` template:

      <p align="center">
        <a href="#"><img src="https://img.shields.io/github/repo-size/{{OUTPUT_REPO_OWNER}}/{{OUTPUT_REPO}}--gh-stats?logo=github" valign="middle" alt="GitHub repo size in bytes" /></a>
      â€¢ <a href="https://github.com/XAMPPRocky/tokei"><img src="https://tokei.rs/b1/github/{{OUTPUT_REPO_OWNER}}/{{OUTPUT_REPO}}--gh-stats?category=lines" valign="middle" alt="lines of text by tokei.rs" /></a>

* Make a merged variant of `gh-action--accum-gh-stats` action script to run altogether the views and the clones counter accumulation

* replace content of `_externals` directory by externals taken out by the `vcstool` (https://github.com/dirk-thomas/vcstool) in these projects:

  - gh-workflow

  Not, it must be taken in a workflow pipeline script using vcstool or it's
  variant.

* Implement a common changelog file accumulation script.

    for:

      /gh-action--accum-repos-changelog
      /gh-workflow

  All changelog files must has a predefined format as explained here:
  `Changelog files organization` : https://gist.github.com/andry81/d278e6d129ca1af326eafb67470a2ae3

  Script uses list of accounts to retrieve list of repositories from each
  account. And then filters list of repositories using include/exclude pattern.

  Then filters each repository by an include path (if has) to a changelog file.
  (default path is a repository root).

  Then use git blame on each changelog file to index each changelog date line
  for an exact commit time to sort changelog blocks between each other.

  Different changelog file parts from different repositories can be separated
  by the line:

  `-------------------------------------------------------------------------------`

  The resulting changelog file can look like this:

    YYYY-MM-DD HH:MM:SS @ account1/repo1/<path-to-changelog-file> # COMMITHASH
    YYYY.MM.DD:
    ...

    YYYY-MM-DD HH:MM:SS @ account1/repo1/<path-to-changelog-file> # COMMITHASH
    YYYY.MM.DD:
    ...

    -------------------------------------------------------------------------------

    YYYY-MM-DD HH:MM:SS @ account1/repo2/<path-to-changelog-file> # COMMITHASH
    YYYY.MM.DD:
    ...

    -------------------------------------------------------------------------------

    YYYY-MM-DD HH:MM:SS @ account2/repo1/<path-to-changelog-file> # COMMITHASH
    YYYY.MM.DD:
    ...

  Where:

    `YYYY.MM.DD:` - a line from a changelog file

    `YYYY-MM-DD HH:MM:SS` date and time of a commit contained respective
    `YYYY.MM.DD:` line from the changelog file

    `COMMITHASH` - respective commit hash

  But, because reading and merging all the changelog files might be a very long
  operation, then there is must be an optimization to that.

  One of the way is use the git blame with the selector to reduce the call
  time down to a limited set of commits or file lines. The downside of it are
  missed changelog file changes above the limit (due to that the changelog file
  can be changed anythere inside). So, we can split all requests into set of
  variants:

  1. Requests with a high limit to call very frequently, for example, - each
     day.

  2. Requests with a moderate limit to call not very frequently, - each week
     end.

  3. Requests without the limit to call rarely, - each month. This the last one
     will synchronize all the missed changes due to requests with the limit.

  This will cover all the cases with out consumption much time from a pipeline.

  But now we have to add the date and time value of the first or last commit
  from the limit to indicate the time when the resulting changelog file part
  has been actual to. Then find this time in the previously merged resulting
  changelog file if the file exists. And update the resulting changlelog file
  instead of a complete rewrite starting from that time.

  For example, if read only first 5 commits from each changelog file, then:

    %% YYYY-MM-DD HH:MM:SS (last merge time)

    YYYY-MM-DD HH:MM:SS @ account1/repo1/<path-to-changelog-file> # COMMITHASH
    YYYY.MM.DD:
    ...

    YYYY-MM-DD HH:MM:SS @ account1/repo1/<path-to-changelog-file> # COMMITHASH
    YYYY.MM.DD:
    ...

    -------------------------------------------------------------------------------

    YYYY-MM-DD HH:MM:SS @ account1/repo2/<path-to-changelog-file> # COMMITHASH
    YYYY.MM.DD:
    ...

    -------------------------------------------------------------------------------

    YYYY-MM-DD HH:MM:SS @ account2/repo1/<path-to-changelog-file> # COMMITHASH
    YYYY.MM.DD:
    ...

    -------------------------------------------------------------------------------

    YYYY-MM-DD HH:MM:SS @ account1/repo1/<path-to-changelog-file> # COMMITHASH
    YYYY.MM.DD:
    ...

    YYYY-MM-DD HH:MM:SS @ account1/repo1/<path-to-changelog-file> # COMMITHASH
    YYYY.MM.DD:
    ...

    YYYY-MM-DD HH:MM:SS @ account1/repo1/<path-to-changelog-file> # COMMITHASH
    YYYY.MM.DD:
    ...

    %% YYYY-MM-DD HH:MM:SS (last merge time)

    YYYY-MM-DD HH:MM:SS @ account1/repo1/<path-to-changelog-file> # COMMITHASH
    YYYY.MM.DD:
    ...

    ...

    -------------------------------------------------------------------------------

    YYYY-MM-DD HH:MM:SS @ account1/repo2/<path-to-changelog-file> # COMMITHASH
    YYYY.MM.DD:
    ...


    YYYY-MM-DD HH:MM:SS @ account1/repo2/<path-to-changelog-file> # COMMITHASH
    YYYY.MM.DD:
    ...


    YYYY-MM-DD HH:MM:SS @ account1/repo2/<path-to-changelog-file> # COMMITHASH
    YYYY.MM.DD:
    ...


    YYYY-MM-DD HH:MM:SS @ account1/repo2/<path-to-changelog-file> # COMMITHASH
    YYYY.MM.DD:
    ...

    %% YYYY-MM-DD HH:MM:SS (last merge time)

    YYYY-MM-DD HH:MM:SS @ account1/repo2/<path-to-changelog-file> # COMMITHASH
    YYYY.MM.DD:
    ...

    ...

    -------------------------------------------------------------------------------

    YYYY-MM-DD HH:MM:SS @ account2/repo1/<path-to-changelog-file> # COMMITHASH
    YYYY.MM.DD:
    ...

    YYYY-MM-DD HH:MM:SS @ account2/repo1/<path-to-changelog-file> # COMMITHASH
    YYYY.MM.DD:
    ...

    YYYY-MM-DD HH:MM:SS @ account2/repo1/<path-to-changelog-file> # COMMITHASH
    YYYY.MM.DD:
    ...

    YYYY-MM-DD HH:MM:SS @ account2/repo1/<path-to-changelog-file> # COMMITHASH
    YYYY.MM.DD:
    ...

    %% YYYY-MM-DD HH:MM:SS (last merge time)

    ...

  Note that the first one line `%% YYYY-MM-DD HH:MM:SS (last merge time)` can
  be omitted and all the rest lines stays as is because will be rewrited by the
  last variant of the request without the limit.

  But now we can read the previously merged resulting file before the rest and
  retrieve the last merge time for each repository changelog file (a new
  repository changelog files wouldn't have any and so will be requested
  completely).

  So we can optimize further down to only 2 variants:

  1. Requests with a limit has been read from the previously merged resulting
     file to call on a regular basis, for example, - each day.

  2. Requests without the limit all the changelog files to remerge all but call
     rarely, - each month.

  And this arises these issues:

    1. If a repository changelog file is new and so not yet been merged in the
       previously merged resulting file.

    2. If a repository changelog file is changed before the last merge time
       been recorded in the previously merged resulting file for the repository
       URL.

  The issue (2) fixes by the (2) request variant.

  The issue (1) can be fixed by splitting the logic into 2 steps:

  1. Collect all new changelog files, git blame them without a limit and merge
     the result with the previously merged resulting file if exists.

     This step will have a minor issue when new
     `%% YYYY-MM-DD HH:MM:SS (last merge time)` records will be unordered with
     the already existed in the previously merged resulting file if exists.

  2. Collect not new changelog files, git blame them with a limit taken from
     previously merged resulting file and merge the result with the last merged
     resulting file.

     This step will have a major issue when the changes before the last merge
     time would NOT be merged into the last merged resulting file. It still can
     be fixed by the (2) request variant.

     Note that the issue can be reduced by increasing the time limit used in
     the git blame. But in that case it brings conflicted records which would
     exist on both sides of a merge.

* Implement a userlog/seclog file generation script from the changelog file
  in the same directory.

    for:

      /gh-action--gen-repos-changelogs
      /gh-workflow

* Implement a changelog/userlog/seclog file synchronization script for all
  sub directories from the same file in the root directory.

    for:

      /gh-action--sync-repos-changelogs
      /gh-workflow
